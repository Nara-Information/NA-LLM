{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "982fbRNdcmyp"
      },
      "outputs": [],
      "source": [
        "# installing required packages\n",
        "# On local machine, please install all of the requirements\n",
        "# with `python>=3.10`\n",
        "\n",
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgt3NF7Jcdix"
      },
      "outputs": [],
      "source": [
        "# loading the model and tokenizer\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM as AutoModel\n",
        "\n",
        "CKPT = \"gyulukeyi/nallm-bart\"\n",
        "model = AutoModel.from_pretrained(CKPT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(CKPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bVggj98c0dx"
      },
      "outputs": [],
      "source": [
        "# setting up the device\n",
        "# BART inference works well with 'cpu'\n",
        "# but for inference on batched texts, 'cuda' acceleartion is better\n",
        "# Optionally use 'mps' on MacOS with Apple Silicon\n",
        "\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available else \\\n",
        "            'mps' if torch.backends.mps.is_available else \\\n",
        "            'cpu'\n",
        "\n",
        "print(\"Will use: \" + device)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61H25pjKeEMs"
      },
      "outputs": [],
      "source": [
        "# set up text\n",
        "# ensure to format the input text for optimal performance\n",
        "# (the format made is the format given to the model while fine-tuning)\n",
        "\n",
        "input = {\n",
        "    \"organization\": \"경찰청\",\n",
        "    \"title\": \"행정심판의 대상\",\n",
        "    \"question\": \"행정심판의 대상은 무엇인가요?\"\n",
        "}\n",
        "\n",
        "input = f\"<{input['organization']}> {input['title']}\\n{input['question']}\"\n",
        "print(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEdhTUaMgkgB"
      },
      "outputs": [],
      "source": [
        "# generate\n",
        "\n",
        "out = model.generate(\n",
        "    input_ids = torch.tensor([tokenizer(input).input_ids]).to(device),\n",
        "    attention_mask = torch.tensor([tokenizer(input).attention_mask]).to(device),\n",
        "    max_new_tokens=120,\n",
        "    top_k=10,\n",
        "    repetition_penalty=1.2,\n",
        ")\n",
        "\n",
        "out = tokenizer.decode(out.squeeze(0),\n",
        "             skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "print(out)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
